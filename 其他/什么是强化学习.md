# 什么是强化学习

强化学习可以用来下围棋。通过在训练过程中给智能体训练大量的棋谱，从而使得智能体能够在新的对局中表现优异。

显然强化学习是通过旧的训练，使得`AGENT`拥有面对**新挑战**的能力，不然如果都是旧环境，那么“学习”的意义在哪里呢？

### 关于“环境”

但是从`env`的角度切入，旧对局中的环境，和新对局中的环境，有很大程度上的**相似性**。这是否意味着，我们在某环境中训练智能体，从而使得智能体能够解决类似环境中的不同情况。

比如这次单机巡航创新场景中的`FlyMovingCircle`，事实证明，我使用圆心按照某个速度，某个角度做匀速直线运动的场景来训练智能体，之后智能体不光能应对这个训练（`CENTER_VELOCIY = 0.1;CENTER_THETA = np.pi/6`）的场景，也能在切换圆心运动速度和运动方向的新场景下有良好的表现。（这符合上面的理解，在某个样例环境中训练，能够解决一系列性质类似的环境中的同类型问题）

在什么样的环境下训练，能泛化到怎么样的类似环境中？这是我不太懂的一个问题。

### 关于“策略”

训练完成后的智能体拥有的是**“决策**”能力。输入为`state`或者`observation`（全局状态中智能体能够观察到的一部分），输出是一个`action`。在最开始的Q学习中，我们这样描述策略：
$$
\pi(s) = argmax_a q(s,a)
$$

### 关于“离散和连续”

当我们完成训练，也就是说：
$$
Q(s,a) \leftarrow r + max Q(\_s,a)
$$
这个关系足够逼近后，我们就认为Q(s,a)这个函数足够“合理”了。

然而上面Q学习的情况中，`state`是有限的，`a`也是有限的，这也是我们能够模拟出所有的Q(s,a)的前提条件，换句话说，二者都是离散的。连续的情况该怎么办呢？

至于连续（无限）的`s`我们使用“函数”思想来解决，但要求`a`是离散的。

至于连续（无限）的动作空间，我们引入一个新的`Actor`网络，相当于一个新的函数，让它来帮我们选择一个动作。

可见**函数思想**是解决连续问题的关键工具。